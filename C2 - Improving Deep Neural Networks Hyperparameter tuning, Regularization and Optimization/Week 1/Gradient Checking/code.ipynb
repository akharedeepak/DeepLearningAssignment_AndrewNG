{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code.ipynb","provenance":[],"authorship_tag":"ABX9TyNiuFSL+Dfnq5oT2wTJoA9i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WRCT-XhqXDk","executionInfo":{"status":"ok","timestamp":1627395307418,"user_tz":240,"elapsed":36233,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}},"outputId":"136cac0c-bcd9-4692-e28a-28f744f30425"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd /content/drive/My\\ Drive/Colab_Notebooks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","/content/drive/My Drive/Colab_Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-Ynb7RyrNWo","executionInfo":{"status":"ok","timestamp":1627395308618,"user_tz":240,"elapsed":1203,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}},"outputId":"8ffe6dc3-9a31-465f-b4e8-1e70b8880658"},"source":["%cd coursera-deep-learning-specialization-master/C2\\ \\-\\ Improving\\ Deep\\ Neural\\ Networks\\ Hyperparameter\\ tuning\\,\\ Regularization\\ and\\ Optimization/Week\\ 1/Gradient\\ Checking"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab_Notebooks/coursera-deep-learning-specialization-master/C2 - Improving Deep Neural Networks Hyperparameter tuning, Regularization and Optimization/Week 1/Gradient Checking\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ggaVJkUArvFX","executionInfo":{"status":"ok","timestamp":1627395400318,"user_tz":240,"elapsed":98,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}}},"source":["import numpy as np\n","from testCases import *\n","from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUKa38-tUqND","executionInfo":{"status":"ok","timestamp":1627395545132,"user_tz":240,"elapsed":105,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}}},"source":["def forward_propagation(x, theta):\n","  J = theta*x\n","  return J"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"syUZlGGwVYYV","executionInfo":{"status":"ok","timestamp":1627395596002,"user_tz":240,"elapsed":111,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}}},"source":["def backward_propagation(x, theta):\n","  dtheta = x\n","  return dtheta"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrY0_nMoVjY0","executionInfo":{"status":"ok","timestamp":1627396140131,"user_tz":240,"elapsed":118,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}}},"source":["def gradient_check(x, theta, epsilon = 1e-7):\n","  thetaplus  = theta + epsilon\n","  thetaminus = theta - epsilon\n","  Jplus  = forward_propagation(x, thetaplus)\n","  Jminus = forward_propagation(x, thetaminus)\n","  gradapprox = (Jplus - Jminus) / (2*epsilon)\n","  grad = backward_propagation(x, theta)\n","\n","  difference = np.linalg.norm(grad - gradapprox) / (np.linalg.norm(grad) + np.linalg.norm(gradapprox))\n","\n","  if difference < 1e-7:\n","    print(\"The gradient is correct!\")\n","  else:\n","    print(\"The gradient is wrong!\")\n","\n","  return difference"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9WIF3KLXRka","executionInfo":{"status":"ok","timestamp":1627396147936,"user_tz":240,"elapsed":100,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}},"outputId":"ff0a6147-e9a7-4226-8b9e-644fe1010744"},"source":["x, theta = 2, 4\n","difference = gradient_check(x, theta)\n","print(\"difference = \" + str(difference))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["The gradient is correct!\n","difference = 2.919335883291695e-10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u0m-P077Xon5","executionInfo":{"status":"ok","timestamp":1627396227503,"user_tz":240,"elapsed":110,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}}},"source":["def forward_propagation_n(X, Y, parameters):\n","    \"\"\"\n","    Implements the forward propagation (and computes the cost) presented in Figure 3.\n","    \n","    Arguments:\n","    X -- training set for m examples\n","    Y -- labels for m examples \n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape (5, 4)\n","                    b1 -- bias vector of shape (5, 1)\n","                    W2 -- weight matrix of shape (3, 5)\n","                    b2 -- bias vector of shape (3, 1)\n","                    W3 -- weight matrix of shape (1, 3)\n","                    b3 -- bias vector of shape (1, 1)\n","    \n","    Returns:\n","    cost -- the cost function (logistic cost for one example)\n","    \"\"\"\n","    \n","    # retrieve parameters\n","    m = X.shape[1]\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","\n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = relu(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = relu(Z2)\n","    Z3 = np.dot(W3, A2) + b3\n","    A3 = sigmoid(Z3)\n","\n","    # Cost\n","    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n","    cost = 1./m * np.sum(logprobs)\n","    \n","    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n","    \n","    return cost, cache"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lUv9jOwX8Cb","executionInfo":{"status":"ok","timestamp":1627397703924,"user_tz":240,"elapsed":100,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}}},"source":["def backward_propagation_n(X, Y, cache):\n","    \"\"\"\n","    Implement the backward propagation presented in figure 2.\n","    \n","    Arguments:\n","    X -- input datapoint, of shape (input size, 1)\n","    Y -- true \"label\"\n","    cache -- cache output from forward_propagation_n()\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","    \n","    dZ3 = A3 - Y\n","    dW3 = 1./m * np.dot(dZ3, A2.T)\n","    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n","    \n","    dA2 = np.dot(W3.T, dZ3)\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    dW2 = 1./m * np.dot(dZ2, A1.T)\n","    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    dW1 = 1./m * np.dot(dZ1, X.T)\n","    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n","    \n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n","                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n","                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"vV1D045oYNfb","executionInfo":{"status":"ok","timestamp":1627397704476,"user_tz":240,"elapsed":106,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}}},"source":["def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n","\n","  # Set-up variables\n","  parameters_values, _ = dictionary_to_vector(parameters)\n","  grad = gradients_to_vector(gradients)\n","  num_parameters = parameters_values.shape[0]\n","  J_plus = np.zeros((num_parameters, 1))\n","  J_minus = np.zeros((num_parameters, 1))\n","  gradapprox = np.zeros((num_parameters, 1))\n","\n","  for i in range(num_parameters):\n","    thetaplus = np.copy(parameters_values)\n","    thetaplus[i] = thetaplus[i] + epsilon\n","    J_plus, _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))\n","\n","    thetaminus = np.copy(parameters_values)\n","    thetaminus[i] = thetaminus[i] - epsilon\n","    J_minus, _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))\n","\n","    gradapprox[i] = (J_plus - J_minus) / (2*epsilon)\n","  \n","  difference = np.linalg.norm(grad-gradapprox) /(np.linalg.norm(grad)+np.linalg.norm(gradapprox))\n","\n","  if difference > 2e-7:\n","      print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n","  else:\n","      print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n","  \n","  return difference"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zlymMo8cHB0","executionInfo":{"status":"ok","timestamp":1627397705025,"user_tz":240,"elapsed":2,"user":{"displayName":"Deepak Akhare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEAbLioRjH-vFkkktO6xoQOZCm1tYRRfKLX_nVsw=s64","userId":"15729154757356582151"}},"outputId":"b683365e-901c-42df-850a-4e6414cf8c65"},"source":["X, Y, parameters = gradient_check_n_test_case()\n","\n","cost, cache = forward_propagation_n(X, Y, parameters)\n","gradients = backward_propagation_n(X, Y, cache)\n","difference = gradient_check_n(parameters, gradients, X, Y)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["\u001b[92mYour backward propagation works perfectly fine! difference = 1.1890913024229996e-07\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a7xKulfYcJYy"},"source":[""],"execution_count":null,"outputs":[]}]}